{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import depthai as dai\n",
    "import blobconverter\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_classes = 21\n",
    "nn_shape = 256\n",
    "\n",
    "def decode_deeplabv3p(output_tensor):\n",
    "    output = output_tensor.reshape(nn_shape,nn_shape)\n",
    "\n",
    "    # scale to [0 ... 255] and apply colormap\n",
    "    output = np.array(output) * (255/num_of_classes)\n",
    "    output = output.astype(np.uint8)\n",
    "    output_colors = cv2.applyColorMap(output, cv2.COLORMAP_TURBO)\n",
    "\n",
    "    # reset the color of 0 class\n",
    "    output_colors[output == 0] = [0,0,0]\n",
    "\n",
    "    return output_colors\n",
    "\n",
    "def show_deeplabv3p(output_colors, frame):\n",
    "    return cv2.addWeighted(frame,1, output_colors,0.4,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = dai.Pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodes hinzufügen und verlinken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im ersten Schritt erstellen wir eine Node für unsere RGB Kamera und konfigurieren diese für ein Segementation Neural Network. Wir setzen die Size auf die gleiche Größe wie der definierte Input des NN. Außerdem wird die Farbreihenfolge von RGB auf BGR umgestellt, da das NN diese Farbreihenfolge erwartet. Um das Programm performanter zu machen wird die Speicherung der Pixel von Interleaved (Bytes werden nach Pixel gespeichert) auf Planar (Bytes werden nach Farbe gespeichert) umgestellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_cam = pipeline.createColorCamera()\n",
    "rgb_cam.setPreviewSize(nn_shape,nn_shape)\n",
    "rgb_cam.setInterleaved(False)\n",
    "rgb_cam.setColorOrder(dai.ColorCameraProperties.ColorOrder.BGR)\n",
    "rgb_cam.setFps(40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im nächsten Schritt erstellen wir eine Node für die Bildsegementierung. Dafür verwenden wir ein vortrainiertes Model vom Model Zoo. Es gibt eigens für DepthAI vortrainierte Modelle die man über den zoo_type auswählen kann. Der Parameter shaves gibt and wieviele der Cores für das NN auf der OAK-D verwendet werden sollen.\n",
    "\n",
    "In diesem Fall verwenden wir das Model Deeplab und setzen ein paar zusätzliche Parameter wie wieviele Frames im Pool gespeichert werden sollen. Ob bei einer vollen Message Queue die ältesten Nachrichten gelöscht oder die Queue blockiert werden soll und wieiviel Threads das NN bekommen soll.\n",
    "\n",
    "Am Ende verlinken wir den Output der vorher definierten Farbkamera mit der NN-Node als Input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = blobconverter.from_zoo(name=\"deeplab_v3_mnv2_256x256\",\n",
    "                                    zoo_type=\"depthai\",\n",
    "                                    shaves=6)\n",
    "segmentation_nn = pipeline.create(dai.node.NeuralNetwork)\n",
    "segmentation_nn.setBlobPath(model_path)\n",
    "segmentation_nn.setNumPoolFrames(4)\n",
    "segmentation_nn.input.setBlocking(False)\n",
    "segmentation_nn.setNumInferenceThreads(2)\n",
    "rgb_cam.preview.link(segmentation_nn.input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im nächsten Schritt erstellen wir ein weiteres NN zur Objektdetection von Gesichtern. Um genauer zu seien handelt es sich hierbei um eine eigene Node (MoblieNetDetectionnetwork). Der Unterschied zur NeuralNetwork Node ist der Output der Node. Ber der NN Node bekommen wir NNData als output bei der MobilenetDetectionNetwork Node bekommen wir Image Detections (label, confidence und boundingbox Koordinaten).\n",
    "\n",
    "Da es sich hierbei aber auch um ein NN handelt müssen wir den Input darauf anpassen. Dafür kreiren wir eine weitere Node ImageManip. Diese dient dazu dass wir den Input der RGB Kamera so manipulieren, dass er für das neuronale Netz geeignet ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<depthai.ImageManipConfig at 0x1531dcbeb30>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face_model = blobconverter.from_zoo(name=\"face-detection-retail-0004\",\n",
    "                                        zoo_type=\"depthai\",\n",
    "                                        shaves=6)\n",
    "face_nn = pipeline.create(dai.node.MobileNetDetectionNetwork)\n",
    "face_nn.setBlobPath(face_model)\n",
    "face_manip = pipeline.createImageManip()\n",
    "face_manip.initialConfig.setResize(300,300)\n",
    "face_manip.initialConfig.setKeepAspectRatio(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Schritt linken wir nun den Input der RGB Kamera der auf das erste Netz angepasst ist mit der ImageManip Node, damit wir den gleichen Input für das zweite NN verwenden können. Dann wird der Output vom ImageManip mit dem Input der Gesichtserkennungs-Node verknüpft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgb_cam.preview.link(face_manip.inputImage)\n",
    "face_manip.out.link(face_nn.input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das XLinkout und setStreamName ist der gleiche Vorgang wie im getting_started Beispiel. Neu hinzugekommen ist passthrough. Mit passthrough haben wir die Möglichkeit den Input in ein NN direkt weiterzuleiten damit dieser in einem anderen Schritt wiederverwendet werden kann.\n",
    "\n",
    "In diesem Beispiel verwenden wir den passthrough dafür, dass wir das RGB Bild mit XLinkOut am Laptop wiedergeben können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "xout_rgb = pipeline.createXLinkOut()\n",
    "xout_rgb.setStreamName(\"rgb\")\n",
    "xout_rgb.input.setBlocking(False)\n",
    "\n",
    "\n",
    "segmentation_nn.passthrough.link(xout_rgb.input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In den nächsten zwei Schritten werden dei XLinkOut für die zwei NN erstellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "xout_nn = pipeline.create(dai.node.XLinkOut)\n",
    "xout_nn.setStreamName(\"nn\")\n",
    "xout_nn.input.setBlocking(False)\n",
    "segmentation_nn.out.link(xout_nn.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xout_face = pipeline.create(dai.node.XLinkOut)\n",
    "xout_face.setStreamName(\"face\")\n",
    "face_nn.out.link(xout_face.input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline laufen lassen\n",
    "Wie im vorgegangenen Beispiel lassen wir die Pipeline nun laufen. Wir kreieren drei Outputstreams (RGB Kamera, Objeksegmentierung und Gesichtserkennung). Lesen von der Gesichtserkennung die Boundingbox aus in dem wir auf die detections des Gesichtserkennungsstreams zugreifen und lassen sie mit cv2.rectangle() zeichnen.\n",
    "\n",
    "Zusätzlich greifen wir auf die Layer des NN zur Objektsegementierung zu und decodieren die Informationen damit wir die Segmentierung über das Bild legen können.\n",
    "\n",
    "Am Ende vergrößern wir das Bild wieder auf 600x600 mit den opencv tool resize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<depthai.ImgDetection object at 0x000001531DC9E8F0>\n",
      "<depthai.ImgDetection object at 0x000001531DC6CD30>\n",
      "<depthai.ImgDetection object at 0x000001531DC02E30>\n",
      "<depthai.ImgDetection object at 0x000001531DCA4AF0>\n",
      "<depthai.ImgDetection object at 0x000001531DCEA030>\n",
      "<depthai.ImgDetection object at 0x000001531DC26630>\n",
      "<depthai.ImgDetection object at 0x000001531DC06130>\n",
      "<depthai.ImgDetection object at 0x000001531DC9E8F0>\n",
      "<depthai.ImgDetection object at 0x000001531DC635B0>\n",
      "<depthai.ImgDetection object at 0x000001531DCC79B0>\n",
      "<depthai.ImgDetection object at 0x000001531DC26630>\n",
      "<depthai.ImgDetection object at 0x000001531DC86B30>\n",
      "<depthai.ImgDetection object at 0x000001531DC6CD30>\n",
      "<depthai.ImgDetection object at 0x000001531DC02E30>\n",
      "<depthai.ImgDetection object at 0x000001531DC546F0>\n",
      "<depthai.ImgDetection object at 0x000001531DC9E8F0>\n",
      "<depthai.ImgDetection object at 0x000001531DCC79B0>\n",
      "<depthai.ImgDetection object at 0x000001531DCA4AF0>\n",
      "<depthai.ImgDetection object at 0x000001531DCC7EB0>\n",
      "<depthai.ImgDetection object at 0x000001531DC26630>\n",
      "<depthai.ImgDetection object at 0x000001531DCCC030>\n",
      "<depthai.ImgDetection object at 0x000001531DCC79B0>\n",
      "<depthai.ImgDetection object at 0x000001531DCC7EB0>\n",
      "<depthai.ImgDetection object at 0x000001531DCCC030>\n",
      "<depthai.ImgDetection object at 0x000001531DC546F0>\n",
      "<depthai.ImgDetection object at 0x000001531DC9E8F0>\n",
      "<depthai.ImgDetection object at 0x000001531DC06130>\n",
      "<depthai.ImgDetection object at 0x000001531DCCC030>\n",
      "<depthai.ImgDetection object at 0x000001531DC86B30>\n",
      "<depthai.ImgDetection object at 0x000001531DC901F0>\n",
      "<depthai.ImgDetection object at 0x000001531DC26630>\n",
      "<depthai.ImgDetection object at 0x000001531DCC79B0>\n",
      "<depthai.ImgDetection object at 0x000001531DCCC030>\n",
      "<depthai.ImgDetection object at 0x000001531DC901F0>\n",
      "<depthai.ImgDetection object at 0x000001531DC6CD30>\n",
      "<depthai.ImgDetection object at 0x000001531DCA4AF0>\n",
      "<depthai.ImgDetection object at 0x000001531DCEA030>\n",
      "<depthai.ImgDetection object at 0x000001531DCC79B0>\n",
      "<depthai.ImgDetection object at 0x000001531DC6CD30>\n",
      "<depthai.ImgDetection object at 0x000001531DCC7EB0>\n",
      "<depthai.ImgDetection object at 0x000001531DCCC530>\n",
      "<depthai.ImgDetection object at 0x000001531DC02E30>\n",
      "<depthai.ImgDetection object at 0x000001531DCC79B0>\n",
      "<depthai.ImgDetection object at 0x000001531DC9E8F0>\n",
      "<depthai.ImgDetection object at 0x000001531DCC7EB0>\n",
      "<depthai.ImgDetection object at 0x000001531DC02E30>\n",
      "<depthai.ImgDetection object at 0x000001531DC26630>\n",
      "<depthai.ImgDetection object at 0x000001531DC86B30>\n",
      "<depthai.ImgDetection object at 0x000001531DC06130>\n",
      "<depthai.ImgDetection object at 0x000001531DCA4AF0>\n",
      "<depthai.ImgDetection object at 0x000001531DCEA030>\n",
      "<depthai.ImgDetection object at 0x000001531DC901F0>\n",
      "<depthai.ImgDetection object at 0x000001531DC26630>\n",
      "<depthai.ImgDetection object at 0x000001531DCCC530>\n",
      "<depthai.ImgDetection object at 0x000001531DCAFE30>\n",
      "<depthai.ImgDetection object at 0x000001531DC901F0>\n",
      "<depthai.ImgDetection object at 0x000001531DCCC030>\n",
      "<depthai.ImgDetection object at 0x000001531DC86B30>\n",
      "<depthai.ImgDetection object at 0x000001531DCCC030>\n",
      "<depthai.ImgDetection object at 0x000001531DC06130>\n",
      "<depthai.ImgDetection object at 0x000001531DC7CA30>\n",
      "<depthai.ImgDetection object at 0x000001531DC546F0>\n",
      "<depthai.ImgDetection object at 0x000001531DCCC530>\n",
      "<depthai.ImgDetection object at 0x000001531DC901F0>\n",
      "<depthai.ImgDetection object at 0x000001531DC86B30>\n",
      "<depthai.ImgDetection object at 0x000001531DCEA030>\n",
      "<depthai.ImgDetection object at 0x000001531DC02E30>\n",
      "<depthai.ImgDetection object at 0x000001531DC6CD30>\n",
      "<depthai.ImgDetection object at 0x000001531DC546F0>\n",
      "<depthai.ImgDetection object at 0x000001531DC901F0>\n",
      "<depthai.ImgDetection object at 0x000001531DC2EFB0>\n",
      "<depthai.ImgDetection object at 0x000001531DCCC030>\n",
      "<depthai.ImgDetection object at 0x000001531DCAFE30>\n",
      "<depthai.ImgDetection object at 0x000001531DCEA030>\n",
      "<depthai.ImgDetection object at 0x000001531DCC79B0>\n",
      "<depthai.ImgDetection object at 0x000001531DCEA030>\n",
      "<depthai.ImgDetection object at 0x000001531DC2EFB0>\n",
      "<depthai.ImgDetection object at 0x000001531DC02E30>\n",
      "<depthai.ImgDetection object at 0x000001531DC6CD30>\n",
      "<depthai.ImgDetection object at 0x000001531DC06130>\n",
      "<depthai.ImgDetection object at 0x000001531DC26630>\n",
      "<depthai.ImgDetection object at 0x000001531DC7CA30>\n",
      "<depthai.ImgDetection object at 0x000001531DC02E30>\n",
      "<depthai.ImgDetection object at 0x000001531DC546F0>\n",
      "<depthai.ImgDetection object at 0x000001531DCEA030>\n",
      "<depthai.ImgDetection object at 0x000001531DC6CD30>\n",
      "<depthai.ImgDetection object at 0x000001531DC9E8F0>\n",
      "<depthai.ImgDetection object at 0x000001531DC546F0>\n",
      "<depthai.ImgDetection object at 0x000001531DC26630>\n",
      "<depthai.ImgDetection object at 0x000001531DC901F0>\n",
      "<depthai.ImgDetection object at 0x000001531DC86B30>\n",
      "<depthai.ImgDetection object at 0x000001531DCC79B0>\n",
      "<depthai.ImgDetection object at 0x000001531DC26630>\n",
      "<depthai.ImgDetection object at 0x000001531DCA4AF0>\n",
      "<depthai.ImgDetection object at 0x000001531DCC79B0>\n",
      "<depthai.ImgDetection object at 0x000001531DCCC030>\n",
      "<depthai.ImgDetection object at 0x000001531DCC7EB0>\n",
      "<depthai.ImgDetection object at 0x000001531DC901F0>\n",
      "<depthai.ImgDetection object at 0x000001531DCCC530>\n",
      "<depthai.ImgDetection object at 0x000001531DC6CD30>\n",
      "<depthai.ImgDetection object at 0x000001531DCCC030>\n",
      "<depthai.ImgDetection object at 0x000001531DCC79B0>\n",
      "<depthai.ImgDetection object at 0x000001531DCEA030>\n",
      "<depthai.ImgDetection object at 0x000001531DCCC030>\n",
      "<depthai.ImgDetection object at 0x000001531DC546F0>\n",
      "<depthai.ImgDetection object at 0x000001531DC901F0>\n",
      "<depthai.ImgDetection object at 0x000001531DCC79B0>\n",
      "<depthai.ImgDetection object at 0x000001531DCEA030>\n",
      "<depthai.ImgDetection object at 0x000001531DCC7EB0>\n",
      "<depthai.ImgDetection object at 0x000001531DC6CD30>\n",
      "<depthai.ImgDetection object at 0x000001531DC9E8F0>\n",
      "<depthai.ImgDetection object at 0x000001531DCA4AF0>\n",
      "<depthai.ImgDetection object at 0x000001531DC06130>\n",
      "<depthai.ImgDetection object at 0x000001531DCC79B0>\n",
      "<depthai.ImgDetection object at 0x000001531DCAFE30>\n",
      "<depthai.ImgDetection object at 0x000001531DCC7EB0>\n",
      "<depthai.ImgDetection object at 0x000001531DCCC530>\n",
      "<depthai.ImgDetection object at 0x000001531DC86B30>\n",
      "<depthai.ImgDetection object at 0x000001531DCA4AF0>\n",
      "<depthai.ImgDetection object at 0x000001531DCCC530>\n",
      "<depthai.ImgDetection object at 0x000001531DC9E8F0>\n",
      "<depthai.ImgDetection object at 0x000001531DC26630>\n",
      "<depthai.ImgDetection object at 0x000001531DC2EFB0>\n",
      "<depthai.ImgDetection object at 0x000001531DCC79B0>\n",
      "<depthai.ImgDetection object at 0x000001531DCCC030>\n",
      "<depthai.ImgDetection object at 0x000001531DC2EFB0>\n",
      "<depthai.ImgDetection object at 0x000001531DCC79B0>\n",
      "<depthai.ImgDetection object at 0x000001531DCCC530>\n",
      "<depthai.ImgDetection object at 0x000001531DCC7EB0>\n",
      "<depthai.ImgDetection object at 0x000001531DC901F0>\n",
      "<depthai.ImgDetection object at 0x000001531DCCC530>\n",
      "<depthai.ImgDetection object at 0x000001531DCAFE30>\n",
      "<depthai.ImgDetection object at 0x000001531DC546F0>\n",
      "<depthai.ImgDetection object at 0x000001531DCCC530>\n",
      "<depthai.ImgDetection object at 0x000001531DC7CA30>\n",
      "<depthai.ImgDetection object at 0x000001531DC2EFB0>\n",
      "<depthai.ImgDetection object at 0x000001531DCCC530>\n",
      "<depthai.ImgDetection object at 0x000001531DCAFE30>\n",
      "<depthai.ImgDetection object at 0x000001531DC7CA30>\n",
      "<depthai.ImgDetection object at 0x000001531DCCC530>\n",
      "<depthai.ImgDetection object at 0x000001531DC06130>\n",
      "<depthai.ImgDetection object at 0x000001531DC9E8F0>\n",
      "<depthai.ImgDetection object at 0x000001531DCCC530>\n",
      "<depthai.ImgDetection object at 0x000001531DCEA030>\n",
      "<depthai.ImgDetection object at 0x000001531DC06130>\n",
      "<depthai.ImgDetection object at 0x000001531DCCC530>\n",
      "<depthai.ImgDetection object at 0x000001531DC02E30>\n",
      "<depthai.ImgDetection object at 0x000001531DCC7EB0>\n",
      "<depthai.ImgDetection object at 0x000001531DC901F0>\n",
      "<depthai.ImgDetection object at 0x000001531DCAFE30>\n",
      "<depthai.ImgDetection object at 0x000001531DCC7EB0>\n",
      "<depthai.ImgDetection object at 0x000001531DCCC030>\n",
      "<depthai.ImgDetection object at 0x000001531DC2EFB0>\n",
      "<depthai.ImgDetection object at 0x000001531DC9E8F0>\n",
      "<depthai.ImgDetection object at 0x000001531DCC7EB0>\n",
      "<depthai.ImgDetection object at 0x000001531DCA4AF0>\n",
      "<depthai.ImgDetection object at 0x000001531DC901F0>\n",
      "<depthai.ImgDetection object at 0x000001531DC06130>\n",
      "<depthai.ImgDetection object at 0x000001531DCCC530>\n",
      "<depthai.ImgDetection object at 0x000001531DC02E30>\n",
      "<depthai.ImgDetection object at 0x000001531DC9E8F0>\n",
      "<depthai.ImgDetection object at 0x000001531DCAFE30>\n",
      "<depthai.ImgDetection object at 0x000001531DC6CD30>\n",
      "<depthai.ImgDetection object at 0x000001531DC26630>\n",
      "<depthai.ImgDetection object at 0x000001531DCAFE30>\n",
      "<depthai.ImgDetection object at 0x000001531DC546F0>\n",
      "<depthai.ImgDetection object at 0x000001531DCCC530>\n",
      "<depthai.ImgDetection object at 0x000001531DC6CD30>\n",
      "<depthai.ImgDetection object at 0x000001531DC02E30>\n",
      "<depthai.ImgDetection object at 0x000001531DC06130>\n",
      "<depthai.ImgDetection object at 0x000001531DCC7EB0>\n",
      "<depthai.ImgDetection object at 0x000001531DCCC030>\n",
      "<depthai.ImgDetection object at 0x000001531DC2EFB0>\n",
      "<depthai.ImgDetection object at 0x000001531DC546F0>\n",
      "<depthai.ImgDetection object at 0x000001531DC7CA30>\n",
      "<depthai.ImgDetection object at 0x000001531DC9E8F0>\n",
      "<depthai.ImgDetection object at 0x000001531DC2EFB0>\n",
      "<depthai.ImgDetection object at 0x000001531DC546F0>\n",
      "<depthai.ImgDetection object at 0x000001531DC9E8F0>\n",
      "<depthai.ImgDetection object at 0x000001531DC7CA30>\n",
      "<depthai.ImgDetection object at 0x000001531DCC79B0>\n",
      "<depthai.ImgDetection object at 0x000001531DCCC530>\n",
      "<depthai.ImgDetection object at 0x000001531DC901F0>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function destroyAllWindows>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "with dai.Device() as device:\n",
    "    device.startPipeline(pipeline)\n",
    "    cam_queue = device.getOutputQueue(name=\"rgb\", maxSize=4, blocking=False)\n",
    "    nn_queue = device.getOutputQueue(name=\"nn\", maxSize=4, blocking=False)\n",
    "    face_queue = device.getOutputQueue(name=\"face\", maxSize=1, blocking=False)\n",
    "\n",
    "    while True:\n",
    "\n",
    "        bbox = None\n",
    "\n",
    "        detection_frame = face_queue.tryGet()\n",
    "\n",
    "        if detection_frame is not None:\n",
    "            detections = detection_frame.detections\n",
    "            if len(detections) > 0 and len(detections) is not None:\n",
    "                print(detections[0])\n",
    "                detection = detections[0]\n",
    "\n",
    "                xmin = max(0, detection.xmin)\n",
    "                ymin = max(0, detection.ymin)\n",
    "                xmax = min(detection.xmax, 1)\n",
    "                ymax = min(detection.ymax, 1)\n",
    "\n",
    "                x = int(xmin*600)\n",
    "                y = int(ymin*600)\n",
    "                w = int(xmax*600-xmin*600)\n",
    "                h = int(ymax*600-ymin*600)\n",
    "\n",
    "                bbox = (x,y,w,h)\n",
    "\n",
    "\n",
    "\n",
    "        color_frame = cam_queue.get()\n",
    "        color_frame = color_frame.getCvFrame()\n",
    "        nn_frame = nn_queue.get()\n",
    "        nn_layers = nn_frame.getAllLayers()\n",
    "        \n",
    "        layer_1 = np.array(nn_frame.getFirstLayerInt32()).reshape(nn_shape,nn_shape)\n",
    "        found_classes = np.unique(layer_1)\n",
    "        output_colors = decode_deeplabv3p(layer_1)\n",
    "\n",
    "        frame = show_deeplabv3p(output_colors, color_frame)\n",
    "        frame = cv2.resize(frame, (600,600))\n",
    "        cv2.rectangle(frame, bbox, (0, 255, 0), 2 )\n",
    "        cv2.putText(frame, \"Found classes {}\".format(found_classes), (2, 10), cv2.FONT_HERSHEY_TRIPLEX, 0.4, (255, 0, 0))\n",
    "        cv2.imshow(\"RGB\", frame)\n",
    "\n",
    "        key = cv2.waitKey(1)\n",
    "        if key == ord('q'):\n",
    "            break\n",
    "\n",
    "cv2.destroyAllWindows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('virt_oak': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "243f874a1232b21151c065a4d170d206e753bdf394db0cee2de6a61f0b5bba72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
